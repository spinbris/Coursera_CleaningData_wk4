---
output: 
  html_document: default
  md_document: default
  pdf_document: default
#always_allow_html: yes
---


```{r setup, include=FALSE}
library(knitr)
#library(kableExtra)
knitr::opts_chunk$set(echo = FALSE)
source("run_analysis.R")
```
# Coursera - John Hopkins University
## Getting and Cleaning Data - Week 4 Assignment

## Project Description

### Overview

The purpose of this assignment is to prepare tidy data from the provided data set which can then be used for further analysis. A summary set of data is also required. The stated purpose of the project is to demonstrate the student's ability to work with, and clean a data set.


### Review Criteria

The following paragraphs address the stated Review Criteria, in terms of how they are intended to be addressed in this assignment submission

#### 1. The submitted data set is tidy

It is perhaps a matter of interpretation as to what constitutes a 'tidy' data set in this instance. The rationale for my approach is detailed later in this document. 


#### 2. The Github repo contains the required scripts.

All (three) scripts have been prepared in RStudio using RStudio Git and Github desktop functionality to then store to my Github account, in this [repository](https://github.com/spinbris/Coursera_CleaningData_wk4)
    
Scripts written are:

 * 'run_analysis.R' - does all data loading, cleaning and output.
 * 'README.Rmd' - queries data and prepares the README.md file (this document). PDF and HTML versions can also be run from the RStudio 'Knit' menu.
 * 'Codebook.Rmd' - similar to above, produces the Codebook.md file (as well as PDF and HTML versions) by running the 'run_analysis.R' script and then querying and assembling appropriate data (augmented by manual text input)

#### 3. GitHub contains a code book that modifies and updates the available codebooks with the data to indicate all the variables and summaries calculated, along with units, and any other relevant information.

As mentioned in previous section the github repository for this assignment contains Codebook in md, html and pdf versions as well as the 'Codebook.Rmd' script which produces and updates them.

#### 4. The README that explains the analysis files is clear and understandable.

The assignment documentation further states the following  in reltion to the README.md file:

*"You should also include a README.md in the repo with your scripts. This repo explains how all of the scripts work and how they are connected."*

This README document is intended to meet this requirement. As mentioned the 'README.Rmd' script which produces/updates this document is also available in the assignment repository. 

#### 5. The work submitted for this project is the work of the student who submitted it.

This work is solely my responsibility, supported by much reading of 

 * Course notes
 * Week 4 discussion forum (notably "Getting and Cleaning Data",David Hood 2015, [link](https://thoughtfulbloke.wordpress.com/2015/09/09/getting-and-cleaning-the-assignment/)) 
 * Hadley Wickham's article[^1] on 'Tidy Data'.
 * Stack Overflow and other Google sources etc
 
 The number of commits to my repository might support this.
 
[^1]: Wickham H.(2014), "Tidy Data", *The Journal of Statistical Software*, vol.59.

Key output documents for this assignment comprise:

  * 'run_analysis.R'   - the R script which reads data and then formats and tidies data.
  * 'all_data.txt'     - output text file containing the 'tidied' data, not summarised.
  * 'summary_data.txt' - output text file, summarised to show average data by subject, activity and signal.
  * 'README.md         - this document, which describes project approach and rationale.
  * 'Codebook.Rmd'      - RMarkdown file which generates and updates the codebooks, as below:
    + 'Codebook.md'     - markdown version
    + 'Codebook.pdf'    - pdf version
  
 
The above are all provided on this Github repo. The original data sources are also saved on repo.

### R Code Prerequisites

Two R packages are required to run the 'run_analysis.R script:

  * tidyverse  - obviously actually quite a few separate packages will load.
  * Data.table - the 'fread' function is used for file reading preference and to allow selective column loads.

In addition, the CodeBook.Rmd and READMe.Rmd scripts use knitR package. 

### Summary Output

#### All_data dataframe

For information, first 6 rows of 'all_data' dataframe are as follows:

```{r all_head, echo=FALSE}
all_data %>%
        head()  %>%
        kable(format = 'markdown') 


```
### Summary_Data dataframe
 

```{r summ_head, echo=FALSE}
summary_data %>%
        head()  %>%
        kable(format = 'markdown') 


```


## Data Set Information

### Source

This assignment uses data collected from the accelerometers from the Samsung Galaxy S smartphone as part of an experiment which is explained in detail on the following web site [(webLink)](http://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones)


The data can be downloaded from the following site, although it is also stored in this repo. [(webLink)](https://d396qusza40orc.cloudfront.net/getdata%2Fprojectfiles%2FUCI%20HAR%20Dataset.zip)

### Overview of Original Data

The original data was produced from an experiment 'Human Activity Recognition Using Smartphones Dataset'[^2] 

[^2]: Davide Anguita, Alessandro Ghio, Luca Oneto, Xavier Parra and Jorge L. Reyes-Ortiz. Human Activity Recognition on Smartphones using a Multiclass Hardware-Friendly Support Vector Machine. International Workshop of Ambient Assisted Living (IWAAL 2012). Vitoria-Gasteiz, Spain. Dec 2012


An overview of the experiment as provided by the related README.txt (which is also contained in the repo for this assignment) is:

"The experiments have been carried out with a group of 30 volunteers within an age bracket of 19-48 years. Each person performed six activities (WALKING, WALKING_UPSTAIRS, WALKING_DOWNSTAIRS, SITTING, STANDING, LAYING) wearing a smartphone (Samsung Galaxy S II) on the waist. Using its embedded accelerometer and gyroscope, we captured 3-axial linear acceleration and 3-axial angular velocity at a constant rate of 50Hz. The experiments have been video-recorded to label the data manually. The obtained dataset has been randomly partitioned into two sets, where 70% of the volunteers was selected for generating the training data and 30% the test data." 

A detailed description of the data is contained in the Codebook, filed in this repo. 

A rationale for using the above as a 'tidy' structure for the assignment is detailed in the CodeBook


## Structure of analysis

### Input Files

Key data input files sourced from the original data source are as follows:

 * 'features.txt'           - List of all features.
 * 'activity_labels.txt'    - Links the class labels with their activity name.
 * 'train/X_train.txt'      - Training set.
 * 'train/y_train.txt'      - Training labels.
 * 'train/subject_train.txt'- subject_ids for each row of train set.
 * 'test/X_test.txt'        - Test set.
 * 'test/y_test.txt'        - Test labels.
 * 'test/subject_test.txt'  - subject_ids for each row if test set.
 
All these files are accessed by 'run_analysis.R' script. 


The other key files used to document the original experiment data sets are:

 * 'README.md' file      - overview of experiment and related data files
 * 'features_info.txt'   - Shows information about the variables used on the feature vector. 
 
 A lot of other files are contained in the source dta folders, but none were needed to complete this assignment, so are not referenced in this document or the CodeBook or any of the scripts.
 
### Tidy Data - Rationale for Structure




 
